
<HTML>
<HEAD>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<link href="https://www.cs.hku.hk/images/hku.png" rel="shortcut icon" />
<title>Lingpeng Kong</title>
    <link rel="stylesheet" href="inde_files/dyerlike.css"/>
    <link rel="stylesheet" href="inde_files/pygments.css"/>
</HEAD>
<BODY>
<header>
<nav>
<ul>
<ul>
    <li><a href="index.html">Home</a></li>
    <li><a href="https://nlp.cs.hku.hk/">Group</a></li>
    <li><a href="publications.html">Publications</a></li>
    <li><a href="contact.html">Contact</a></li>
</ul>

</ul>
</nav>
<h2>Lingpeng Kong</h2> 
</header>

<section>
<!-- <h2> Note </h2> -->
<!-- <br> -->
Starting from 2024, I have decided to discontinue the use of the <em>list of publications</em> format, as I believe that it fails to provide an intuitive view of our research. Instead, I am creating this active, self-curated selection of papers. I have intentionally excluded the publication venue information from these papers, which means they may be arXiv preprints or peer-reviewed conference / journal papers. For that information, kindly refer to my <a href="lingpenk_cv.pdf">resume.</a> The selection only represents my personal subjective preferences.

</section>

<section>
In this group of papers, we explore text diffusion models.
<br>
    <li> [2024] <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, <a href="https://summmeer.github.io/">Shansan Gong</a>*, Liheng Chen*, <a href="https://lzhengisme.github.io/">Lin Zheng</a>, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://han-shi.github.io/">Han Shi</a>, <a href="https://i.cs.hku.hk/~cwu/">Chuan Wu</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Wei Bi</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2402.07754"> Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a></strong>.</li>

    <li> [2023] <a href="https://lzhengisme.github.io/">Lin Zheng</a>, Jianbo Yuan, Lei Yu, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2302.05737"> A Reparameterized Discrete Diffusion Model for Text Generation</a></strong>.</li>

    <li> [2023] <a href="https://summmeer.github.io/">Shansan Gong</a>, <a href="https://scholar.google.com/citations?user=BizedOAAAAAJ">Mukai Li</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2310.05793">Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models</a></strong>.</li>


     <li> [2023] <a href="https://summmeer.github.io/">Shansan Gong</a>, <a href="https://scholar.google.com/citations?user=BizedOAAAAAJ">Mukai Li</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2210.08933"> DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</a></strong>.</li>
 
      
</section>

<section>
In this group of papers, we explore new transformer architectures with the goal of enhancing efficiency and performance in long sequence modeling.
<br>
     <li> [2023] <a href="https://lzhengisme.github.io/">Lin Zheng</a>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ">Jianbo Yuan</a>, <a href="https://chongw.github.io/">Chong Wang</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2302.04542"> Efficient Attention via Control Variates</a></strong>.</li>

     <li> [2022] <a href="https://lzhengisme.github.io/">Lin Zheng</a>, <a href="https://chongw.github.io/">Chong Wang</a>, and Lingpeng Kong, <strong><a href=https://arxiv.org/abs/2204.04667> Linear Complexity Randomized Self-attention Mechanism</a></strong>.</li>

     <li> [2022] <a href="https://lzhengisme.github.io/">Lin Zheng</a>, <a href="https://hk.linkedin.com/in/huijie-pan-8a0a871a4">Huijie Pan</a>, and Lingpeng Kong, <strong><a href=https://arxiv.org/abs/2110.02453> Ripple Attention for Visual Perception with Sub-quadratic Complexity</a></strong>.</li>

     <li> [2021] <a href="https://homes.cs.washington.edu/~hapeng/">Hao Peng</a>, <a href="https://nik0spapp.github.io/">Nikolaos Pappas</a>, <a href="https://dyogatama.github.io/">Dani Yogatama</a>, <a href="https://schwartz-lab-huji.github.io/">Roy Schwartz</a>, <a href="https://homes.cs.washington.edu/~nasmith/">Noah A. Smith</a>, and Lingpeng Kong, <strong><a href=https://openreview.net/forum?id=QtTKTdVrFBB>Random Feature Attention</a></strong>.</li>

The evaluation of these architectures.
    <li> [2023] <a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>*, <a href="https://scholar.google.com/citations?user=slwTiOUAAAAJ">Shuyang Jiang</a>*, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://lzhengisme.github.io/">Lin Zheng</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2210.07661"> CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling</a></strong>.</li>
</section>

<section>
In this group of papers, we explore key issues related to large language models (LLMs).
<br><br>
In-context Learning.
    <li> [2023] <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>*, <a href="https://scholar.google.com/citations?hl=en&user=7e_BZuYAAAAJ">Yaoxiang Wang</a>*, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2212.10375">Self-adaptive In-context Learning</a></strong>.</li>
     <li> [2023] <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://taoyds.github.io">Tao Yu</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2302.05698">Compositional Exemplars for In-context Learning</a></strong></li>
Principles of Data Synthesis.
     <li> [2023] <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://pipilurj.github.io/">Renjie Pi</a>, <a href="https://linyongver.github.io/yonglin.github.io/">Lin Yong</a>, <a href="https://xuhangcn.github.io/">Hang Xu</a>, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, <a href="https://scholar.google.com/citations?user=qd06pUgAAAAJ">Weizhong Zhang</a>, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong, <strong><a href="https://openreview.net/pdf?id=h5OpjGd_lo6"> Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning</a></strong>.</li>

    <li> [2022] <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://taoyds.github.io">Tao Yu</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2210.12329"> ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback</a></strong>.</li>

    <li> [2023] <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://scholar.google.com/citations?user=t_Bwt70AAAAJ">Chengzu Li</a>, Lingpeng Kong, <a href="https://taoyds.github.io">Tao Yu</a>, <strong><a href=https://arxiv.org/abs/2305.13917>Generating Data for Symbolic Language with Large Language Models</a></strong>.</li>

    <li> [2022] <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>*, <a href="https://qtli.github.io/">Qintong Li</a>, <a href="https://xuhangcn.github.io/">Hang Xu</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, <a href="https://taoyds.github.io">Tao Yu</a>, and Lingpeng Kong, <strong><a href=https://arxiv.org/abs/2202.07922> ZeroGen: Efficient Zero-shot Learning via Dataset Generation</a></strong>.</li>
Ultra-long Sequences.
    <li> [2023] <a href="https://scholar.google.com/citations?user=fY69CxIAAAAJ">Chenxin An</a>, <a href="https://sites.google.com/view/fei-huang">Fei Huang</a>, <a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>, <a href="https://summmeer.github.io/">Shansan Gong</a>, <a href="https://xpqiu.github.io/en.html">Xipeng Qiu</a>, <a href="https://scholar.google.com/citations?user=QeSoG3sAAAAJ">Chang Zhou</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2402.17463">Training-Free Long-Context Scaling of Large Language Models</a></strong>.</li>

    <li> [2023] <a href="https://scholar.google.com/citations?user=fY69CxIAAAAJ">Chenxin An</a>, <a href="https://summmeer.github.io/">Shansan Gong</a>, <a href="https://maszhongming.github.io/">Ming Zhong</a>, <a href="https://scholar.google.com/citations?user=EEXx2H4AAAAJ">Xingjian Zhao</a>, <a href="https://scholar.google.com/citations?user=BizedOAAAAAJ">Mukai Li</a>, <a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>, Lingpeng Kong, and <a href="https://xpqiu.github.io/en.html">Xipeng Qiu</a>, <strong><a href="https://arxiv.org/abs/2307.11088">L-Eval: Instituting Standardized Evaluation for Long Context Language Models</a></strong>.</li>
    
</section>

<section>
In this group of papers, we explore computational science enabled by AI.
<br><br>
Mathematics.
<li> [2024] <a href="https://qtli.github.io/">Qintong Li</a>, <a href="https://nealcly.github.io/">Leyang Cui</a>, <a href="https://scholar.google.com/citations?user=h-87C9cAAAAJ">Xueliang Zhao</a>, Lingpeng Kong, and <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Wei Bi</a>, <strong><a href="https://arxiv.org/abs/2402.19255">GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers</a></strong>.</li>

<li> [2023] <a href="https://scholar.google.com/citations?user=h-87C9cAAAAJ">Xueliang Zhao</a>, <a href="https://wenda302.github.io/">Wenda Li</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2305.16366">Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving</a></strong>.</li>

<li> [2023] <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://pipilurj.github.io/">Renjie Pi</a>, <a href="https://2003pro.github.io/">Jipeng Zhang</a>, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://zhongwanjun.github.io/">Wanjun Zhong</a>, <a href="https://scholar.google.com/citations?user=gFoSqqkAAAAJ">Yufei Wang</a>, <a href="https://scholar.google.com.sg/citations?user=2p7x6OUAAAAJ">Lanqing Hong</a>, <a href="https://scholar.google.com/citations?user=OEPMQEMAAAAJ">Jianhua Han</a>, <a href="https://xuhangcn.github.io/">Hang Xu</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2312.11370">G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</a></strong>.</li>

<li> [2023] <a href="https://scholar.google.com/citations?user=h-87C9cAAAAJ">Xueliang Zhao</a>, <a href="https://timhuang1.github.io/">Xinting Huang</a>, <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Wei Bi</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2310.12960">SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving</a></strong>.</li>

Biology and Chemistry.
<li> [2023] <a href="https://chang-github-00.github.io/-changma/">Chang Ma</a>, <a href="https://zhao-ht.github.io/haitengzhao/">Haiteng Zhao</a>, <a href="https://lzhengisme.github.io/">Lin Zheng</a>, <a href="https://scholar.google.com/citations?user=i2Zn_I0AAAAJ">Jiayi Xin</a>, <a href="https://qtli.github.io/">Qintong Li</a>, Lijun Wu, Zhihong Deng, Yang Lu, <a href="https://leuchine.github.io/">Qi Liu</a>, and Lingpeng Kong, <strong><a href="https://arxiv.org/abs/2302.12563">Retrieved Sequence Augmentation for Protein Representation Learning</a></strong>.</li>

<li> [2023] <a href="https://zhao-ht.github.io/haitengzhao/">Haiteng Zhao</a>, Shengchao Liu, <a href="https://chang-github-00.github.io/-changma/">Chang Ma</a>, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, <a href="https://leuchine.github.io/">Qi Liu</a>, <strong><a href="https://arxiv.org/abs/2306.13089">GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning</a></strong>.</li>
</section>

</body>
</html>

