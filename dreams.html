
<HTML>
<HEAD>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<link href="https://www.cs.hku.hk/images/hku.png" rel="shortcut icon" />
<title>Lingpeng Kong</title>
    <link rel="stylesheet" href="inde_files/dyerlike.css"/>
    <link rel="stylesheet" href="inde_files/pygments.css"/>
</HEAD>
<BODY>
<header>
<nav>
<ul>
<ul>
    <li><a href="index.html">Home</a></li>
    <li><a href="https://nlp.cs.hku.hk/">Group</a></li>
    <li><a href="publications.html">Publications</a></li>
    <li><a href="contact.html">Contact</a></li>
</ul>

</ul>
</nav>
<h2>Lingpeng Kong</h2> 
</header>

<section>
   This page summarizes our work on diffusion reasoning models (DREAMs).
</section>


<section>
    Foundation.
    <br>
    <li> [COLM 2024] <a href="https://arxiv.org/abs/2302.05737">A Reparameterized Discrete Diffusion Model for Text Generation</a>. The framework for discrete diffusion models that improves text generation through better training and sampling techniques.</li> 
    <li> [ICLR 2023] <a href="https://arxiv.org/abs/2210.08933">DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</a>. DiffuSeq established a paradigm for using text diffusion models to handle sequence-to-sequence tasks.</li>
</section>

<section>
    Reasoning.
    <br>
    <li> [ICLR 2025] <a href="https://arxiv.org/abs/2410.14157">Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</a>. Through studying subgoal learning, we show diffusion models significantly outperform autoregressive models on reasoning tasks by better handling difficult subgoals.</li> 
    <li> [ICLR 2025] <a href="https://arxiv.org/abs/2210.08933">Implicit Search via Discrete Diffusion: A Study on Chess</a>. DiffuSearch uses discrete diffusion to enable implicit search capabilities in language models, outperforming both searchless and MCTS-enhanced approaches in chess-related tasks.</li>
    <li> [NeurIPS 2024] <a href="https://arxiv.org/abs/2402.07754">Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a>. Chain-of-Thought Reasoning, but in the text diffusion world.</li>
</section>

<section>
    Scaling.
    <li> [ICLR 2025] <a href="https://arxiv.org/abs/2410.17891">Scaling Diffusion Language Models via Adaptation from Autoregressive Models</a>. A method to convert pre-trained autoregressive models like GPT-2 and LLaMA into competitive diffusion models through continual pre-training, making large-scale diffusion language models more accessible.</li> 
</section>

</body>
</html>

